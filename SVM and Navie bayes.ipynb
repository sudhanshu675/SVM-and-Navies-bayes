{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7aacf-d6fd-4292-8e31-c5ee6bdee8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q 1: What is Information Gain, and how is it used in Decision Trees?\n",
    ":-   ​Information Gain (IG) is a measure used in the construction of Decision Trees to determine the effectiveness of a feature in separating the training data according to its target classes.\n",
    "     It quantifies the reduction in entropy (or impurity) that results from splitting a dataset based on a particular feature\n",
    "     .How it's used:\n",
    "     ​Decision Trees are built top-down. At each node, the algorithm calculates the Information Gain for every available feature.\n",
    "     ​The feature that yields the highest Information Gain (meaning it causes the biggest drop in impurity/entropy) is chosen as the splitting criterion for that node.\n",
    "     ​This process is repeated recursively until a stopping condition is met. The goal is to maximize the purity of the resulting child nodes.\n",
    "\n",
    "#Q 2: What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    ":-   ​Gini Impurity and Entropy are both measures used to quantify the impurity or randomness in a set of data (a node in a Decision Tree).\n",
    "    The main difference lies in their calculation and properties:\n",
    "Feature\n",
    "Gini Impurity\n",
    "Entropy\n",
    "Formula\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "Calculation\n",
    "Measures the probability of incorrectly classifying a randomly chosen element.\n",
    "Measures the degree of randomness or uncertainty in the data.\n",
    "Range\n",
    "[0, 0.5]\n",
    "[0, 1] (for binary classification)\n",
    "Computational Cost\n",
    "Generally faster to compute as it avoids logarithmic calculations.\n",
    "Involves logarithms, making it slightly slower to compute.\n",
    "Preferred Split\n",
    "Aims to minimize Gini Impurity.\n",
    "Aims to minimize Entropy, which maximizes Information Gain.\n",
    "Use Case\n",
    "Used in algorithms like CART (Classification and Regression Trees).\n",
    "Used in algorithms like ID3 and C4.5.\n",
    "    Both measures achieve a similar goal: a value of zero indicates a perfectly pure node (all samples belong to the same class), while a higher value indicates a less pure node.\n",
    "       \n",
    "#Q 3: What is Pre-Pruning in Decision Trees?\n",
    "\n",
    ":-   ​Pre-Pruning is a technique used to prevent a Decision Tree from growing too large and overfitting the training data.\n",
    "      It involves stopping the tree construction early by imposing constraints or conditions on the tree's growth before a split is made at a node.\n",
    "     ​Common pre-pruning techniques/stopping criteria include:\n",
    "     ​Maximum Tree Depth: Limiting the maximum number of levels (e.g., max_depth=5).\n",
    "     ​Minimum Samples for a Split: Requiring a minimum number of samples in a node before a split can be considered (e.g., min_samples_split=20).\n",
    "     ​Minimum Samples in a Leaf Node: Requiring a minimum number of samples for any new leaf node that would be created by a split (e.g., min_samples_leaf=10).\n",
    "     ​Maximum Impurity Decrease (or Minimum Information Gain): Stopping if the split doesn't improve impurity by at least a certain threshold (e.g., min_impurity_decrease).\n",
    "     ​Advantage: Pre-pruning is computationally less expensive than Post-Pruning (where you grow the full tree and then cut back).\n",
    "      Disadvantage: It can sometimes stop the tree from finding a split that, while not immediately beneficial,\n",
    "      leads to much better splits later on, potentially resulting in an underfitted model.\n",
    "\n",
    "#Q 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
    ":-   \n",
    "    import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the dataset (using Iris for a practical example)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# 2. Split data into training and testing sets (optional but good practice)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize the Decision Tree Classifier with Gini criterion\n",
    "# The criterion='gini' is the default, but we explicitly set it.\n",
    "dtc = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 5. Get and print the feature importances\n",
    "importances = dtc.feature_importances_\n",
    "\n",
    "print(\"Feature Importances for Decision Tree (Gini Criterion):\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    # Print the importance for each feature, formatted as a percentage\n",
    "    print(f\"{name}: {importance*100:.2f}%\")\n",
    "\n",
    "# 6. Evaluate the model (optional)\n",
    "accuracy = dtc.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy on Test Set: {accuracy*100:.2f}%\")\n",
    "         Feature Importances for Decision Tree (Gini Criterion):\n",
    "sepal length (cm): 0.00%\n",
    "sepal width (cm): 0.00%\n",
    "petal length (cm): 91.13%\n",
    "petal width (cm): 8.87%\n",
    "\n",
    "Model Accuracy on Test Set: 100.00%\n",
    "  .\n",
    "#Q5: What is a Support Vector Machine (SVM)?\n",
    "\n",
    ":-  A Support Vector Machine (SVM) is a powerful and versatile supervised machine learning algorithm used for both classification and regression tasks, though primarily for classification.\n",
    "    The core idea of SVM is to find an optimal hyperplane that distinctly separates the data points of different classes in the feature space.\n",
    "    The \"optimal\" hyperplane is the one that has the largest margin—the maximum distance between the hyperplane and the nearest data points of any class.\n",
    "    Support Vectors: These are the data points that lie closest to the hyperplane (on the margin). They are the most crucial elements of the dataset,\n",
    "    as they directly influence the position and orientation of the optimal hyperplane.\n",
    "    Margin: The distance between the hyperplane and the support vectors. SVM aims to maximize this margin for better generalization.\n",
    "    SVM is particularly effective in high-dimensional spaces and when the classes are not linearly separable, by using the Kernel Trick (Question 6).\n",
    "\n",
    "#Q 6: What is the Kernel Trick in SVM?\n",
    "\n",
    ":-   The Kernel Trick is a fundamental technique that allows SVM to effectively handle non-linearly separable data without explicitly transforming the data into a higher-dimensional space.\n",
    "     The Problem: Many real-world classification problems involve data that cannot be separated by a simple straight line (a linear hyperplane) in the original feature space.\n",
    "     The Solution (The \"Trick\"): Instead of computationally expensive mapping the data points \\mathbf{x} to a higher-dimensional feature space \\phi(\\mathbf{x}),\n",
    "     the Kernel Trick uses a Kernel Function (e.g., RBF, polynomial, sigmoid) to calculate the dot product of the data points as if they were already in that higher-dimensional space.\n",
    "     The Benefit: The Kernel Function K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}_j) directly computes the similarity between two points in the high-dimensional \n",
    "     space without ever calculating the coordinates \\phi(\\mathbf{x}) themselves. This significantly reduces computational complexity while still allowing a linear decision boundary \n",
    "     to be found in the new, higher-dimensional space.\n",
    "\n",
    "#Q7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
    ":-     \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "# Standardizing the data is often recommended for SVM but not strictly required for this problem.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train the Linear SVM Classifier\n",
    "svc_linear = SVC(kernel='linear', random_state=42)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svc_linear.predict(X_test)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# 4. Train the RBF (Radial Basis Function) SVM Classifier\n",
    "svc_rbf = SVC(kernel='rbf', random_state=42) # RBF is the default kernel\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svc_rbf.predict(X_test)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# 5. Compare Accuracies\n",
    "print(\"--- SVM Kernel Accuracy Comparison (Wine Dataset) ---\")\n",
    "print(f\"Linear Kernel Accuracy: {accuracy_linear*100:.2f}%\")\n",
    "print(f\"RBF Kernel Accuracy:    {accuracy_rbf*100:.2f}%\")\n",
    "        --- SVM Kernel Accuracy Comparison (Wine Dataset) ---\n",
    "Linear Kernel Accuracy: 98.15%\n",
    "RBF Kernel Accuracy:    70.37%\n",
    "            \n",
    "#Q 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "\n",
    " :-  The Naïve Bayes classifier is a family of probabilistic machine learning algorithms based on applying Bayes' theorem with a \"naïve\" assumption of feature independence.\n",
    "     Where C is the class and X is the vector of features.\n",
    "     Core Function: It calculates the probability of an observation belonging to a certain class C given its feature values X, and then predicts\n",
    "     the class with the highest probability (Maximum A Posteriori hypothesis).\n",
    "     Why is it called \"Naïve\"?\n",
    "     The classifier is called \"Naïve\" because it makes the simplifying, yet often effective, assumption that all features are conditionally independent given the class.\n",
    "     In reality, features are rarely perfectly independent (e.g., a person's height and weight are related). \n",
    "     However, this assumption drastically simplifies the computation, making Naïve Bayes models very fast to train and highly efficient,\n",
    "     especially for high-dimensional data like text classification.\n",
    "\n",
    "#Q 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
    "\n",
    ":-    These are the three most common variants of the Naïve Bayes classifier, distinguished by the underlying distribution they assume for the features (P(X|C)):\n",
    "    \n",
    "1. Gaussian Naïve Bayes\n",
    "Used for continuous numeric features.\n",
    "Assumes data follows a normal distribution.\n",
    "Applications: Iris dataset, medical data, sensor data.\n",
    "\n",
    "2. Multinomial Naïve Bayes\n",
    "\n",
    "Used for count data.\n",
    "Feature values must be non-negative integers.\n",
    "Ideal for:\n",
    "✔ text classification\n",
    "✔ bag-of-words\n",
    "✔ term frequency counts\n",
    "\n",
    "3. Bernoulli Naïve Bayes\n",
    "For binary features (0/1).\n",
    "Features represent presence or absence of something.\n",
    "Example:\n",
    "✔ Email spam classification with 0/1 indicators\n",
    "✔ Word present or not present\n",
    "\n",
    "Key Differences Table\n",
    "Type Data Type Best For\n",
    "Gaussian NB Continuous values Sensor, numeric datasets\n",
    "Multinomial NB Counts (integers) NLP text classification\n",
    "Bernoulli NB Binary (0/1) Spam detection, document classification\n",
    "         \n",
    "#Q10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
    "          \n",
    ":- from sklearn.datasets import load_breast_cancer\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.naive_bayes import GaussianNB\n",
    "   from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Breast Cancer dataset\n",
    "   data = load_breast_cancer()\n",
    "   X = data.data\n",
    "   y = data.target\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize the Gaussian Naive Bayes Classifier\n",
    "   gnb = GaussianNB()\n",
    "\n",
    "# 4. Train the model\n",
    "   gnb.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions\n",
    "   y_pred = gnb.predict(X_test)\n",
    "\n",
    "# 6. Evaluate accuracy\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "  print(\"--- Gaussian Naïve Bayes on Breast Cancer Dataset ---\")\n",
    "  print(f\"Number of test samples: {len(X_test)}\")\n",
    "  print(f\"Accuracy Score: {accuracy*100:.2f}%\")\n",
    "           --- Gaussian Naïve Bayes on Breast Cancer Dataset ---\n",
    "  Number of test samples: 171\n",
    "  Accuracy Score: 93.57%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
